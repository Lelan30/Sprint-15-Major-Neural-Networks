### Describe Nerual Network Used for Modeling Sequence ###
# Recurrent Neural Netoworks (RNN)
# Long Short-Term Memory (LSTM)

## Sequence: ##
'''
A collection of objects (integers, floats, characters, tokens, etc,
where you can repeat the order of matter and objects.
(A Python list is an example, and NumPy Arrays)
'''

## Time Series: ##
'''
Data with a continuous marker for where points lie in time,
such as a date, a timestamp, Unix time, etc...
(Weather Forecasting is an example)
'''

## Recursion: ##
'''
Defining objects based on previously defined objects of the same type,
ie... When things call themselves one or more times over
(Pascals Triangle) - Each number is the sum of the two numbers above it
'''

## Recurrent Neural Network: ##
'''
A Layer where the output from the nodes feed back into itself,
(Recurrent Layer)
Simple RNN have a Vanishing Gradient Problem:
results in back-propagation either exploding(becoming very large) or,
vanishing(becoming very small)
'''

## Long Short-Term Memory: ##
'''
Create a memory state(save point) within network that adds to gradient,
(This prevents vanishing)
'''

## Follow-Along ##
# Imports
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Instantiate
model = keras.Sequential()
model.add(layers.Embedding(input_dim=1000, output_dim=64))

# Output of Simple RNN 2D tensor of shape
model.add(layers.SimpleRNN(128))

# Additional hidden layer
model.add(layers.Dense(10))

# View architecture
model.summary()

'''
output:

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 64)          64000     
_________________________________________________________________
simple_rnn (SimpleRNN)       (None, 128)               24704     
_________________________________________________________________
dense (Dense)                (None, 10)                1290      
=================================================================
Total params: 89,994
Trainable params: 89,994
Non-trainable params: 0
_________________________________________________________________
'''

# Create Network with Long Short-Term Memory layer (LSTM)

# LSTM network example
model = keras.Sequential()
# Add an Embedding layer input vocab of size 1000, output 64
model.add(layers.Embedding(inout_dim=1000, output_dim=64))

# Add LSTM layer with 128 units
model.add(layers.LSTM(128))

# Add Dense layers of 10
model.add(layers.Dense(10))

model.summary()

'''
output:

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, None, 64)          64000     
_________________________________________________________________
lstm (LSTM)                  (None, 128)               98816     
_________________________________________________________________
dense_1 (Dense)              (None, 10)                1290      
=================================================================
Total params: 164,106
Trainable params: 164,106
Non-trainable params: 0
_________________________________________________________________
'''
